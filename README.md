# ReddiKnowDense: Detecting Signs of Mental Illness from Reddit Comments
### Summary
- This project develops an <b>end-to-end machine learning pipeline</b> that leverages the advanced capabilities of RoBERTa, a large language model, for text classification.
- The model is <b>fine-tuned using two distinct classification heads</b>: a Multi-Layer Perceptron (MLP) and a Convolutional Neural Network (CNN), labeled as RoBERTa-MLP and RoBERTa-CNN, respectively.
- This approach allows for a <b>comparative analysis of these methods with GPT 3.5's in-context learning capabilities</b>, focusing on <b>identifying potential mental illness indicators in Reddit comments</b>, a novel real world problem.
- Bonus: Along the way, the <b>power behind the transformer architecture</b> will be discussed in a way to make it understandable to the masses. My goal here is to <b>help people struggling to get to grips with modern NLP techinques, gain a more intuitive understanding of why these methods are so effective</b> rather than just using them as a so called black box.
- However, we will start with the approach to give you and idea the project workflow before delving into this explanation section.

NB: This project also explores the <b>advanced machine learning concept of self-training, where a model's own predictions are recycled as training data to enhance performance</b>. This method is particularly effective with small datasets, where the classification decision boundary is ambiguous. By continuously retraining with its predictions, the model fine-tunes this boundary, improving its accuracy. I will omit this part of the project from this repo to avoid it becoming too convoluted but may make another one in future detailing this method and its effectivness in this domain.

NB: You will notice <b>I have left the API keys I used to connect to the Reddit API within the scripts</b>. Of course these keys have been deleted now and so will not work if you try to use them, but serve as an example of the keys you will need when registering for API usage. Note that in a commercical setting these keys would need to be stored as environment variables to mitigate security risks. This project was purely academic for me, hence I just used them within the script.

NB: The following is a snapshot of the project, highlight the key points, findings and analysis used. It is taken from a much longer report produced by me during the completion of my masters degree. As such I have included the whole reference list at the end, but only part of this is spoken about in this repo.

### Approach
#### Data Acquisition and Pre-Processing
- The data for this project came from Reddit, an online social media platform where users discuss various topics in forums known as subreddits. Using the PMAW API, a Python tool, data was gathered data from specific subreddits proven relevant for mental heath diagnosis in numerous studies (Gkotsis et al., 2017),  (Cohan et al., 2018),  (Jiang et al., 2020) . In order to enhance data quality, only comments from users who self-reported (Coppersmith et al., 2014) their conditions were used, ensuring accuate ground truth labels were attained for the dataset. Below is a table of all Subreddits used.
- 
<img width="1300" alt="image" src="https://github.com/Adam-R26/ReddiKnowDense-Detecting-Signs-of-Mental-Illness-from-Reddit-Comments/assets/53123097/1b0c78b4-8750-4dee-a8cf-86b3485703f1">
  
- Data was collected from a total of 17 subreddits covering 9 different themes from the period spanning 2015-2023, as is outlined in the table below. The initial plan was to acquire 50000 submissions for each theme. However, in practice this was unachievable due to an insufficient number of posts existing for certain subreddits to meet this threshold. This was particularly an issue for the eating disorder related subreddits, hence the additional ‘r/bulimia’ subreddit  was included and the date range widened to span 2010-2023, in order to acquire additional data. A further extraneous factor to note, is that there were active issues limiting the amount of data that could be acquired through PMAW at the time of development of this project. This also limited the amount of data that was able to be acquired efficiently as the alternative approaches had rate limits imposed upon them and so were not recommended for 
building large datasets. Below is a table detailing the amount of data extracted from each subreddit.

<img width="1300" alt="image" src="https://github.com/Adam-R26/ReddiKnowDense-Detecting-Signs-of-Mental-Illness-from-Reddit-Comments/assets/53123097/5c787fde-32b1-47a7-918f-98d1ab591215">



- Text preprocessing within the project served two main purposes. The first purpose was to derive accurate ground￾truth labels for each submission within the dataset, and the second was to transform the data into a suitable format for consumption by the RoBERTa model. Expanding upon the derivation of ground-truth, we note that the raw data obtained from Reddit contained notable noise i.e., submissions from individuals not bearing a mental disease. The source of this noise was varied, with some common issues being users asking about the conditions of family members, individuals trying to learn more about the illnesses etc. In order to distinguish posts from those users suffering with a mental illness from ones enquiring about the disease, the approach of self-reporting combined with a domain expert labelled keyword list was used. Self-reporting outlined in (Coppersmith et al., 2014), is the process of using regular expressions to search through the submissions of users to determine whether or not they mention that they have been diagnosed with the illness that they are posting about. The subset of submissions from the users who mentioned their diagnosis are then taken, and assigned a label based on the subreddit the submission came from to form the dataset. i.e., if a submission comes from the ‘r/depression’ subreddit, then it would be assigned a label of ‘depression’. While the self-reporting method exclusively uses comments from those users who mention their diagnosis, other papers in the area such as (Jiang et al., 2020), include users who have specific domain-related keywords within one of their comments. Hence it was decided to combine the approaches in order to maximize the amount of labelled data within the resultant dataset. The exact keywords used in the regular expressions used for the 
self-reporting aren’t explicitly mentioned within the related literature, hence appropriate keywords were deduced using intuition. The domain expert labelled keyword lists however were available and can be viewed in the below figures. (Keywords from both figures were combined to create the keyword list used in this project.)

- Discussing the second purpose of pre-processing, we note that in order to obtain meaningful feature vectors from BERT based models it is important to maintain as much context within the text as possible. This is because BERT derived feature vectors are affected by the words surrounding each word. Therefore, the aim of preprocessing is to clarify the context within the natural language via removing anything that disrupts the natural flow of text. Building upon this intuition consider the steps outlined in figure 6. Removing deleted submissions and restricting to self￾reported users removes noise from the training data increasing its quality. In addition to this, removing short submissions and combining post titles and text increases the likelihood of mental health related linguistic signals  lying within the text. Moreover, by removing Reddit usernames, special characters, URLs etc. We remove unimportant non-contextual data from our text. Finally, a subset of the highest upvoted posts is taken to form the overall dataset. The reasoning for this step is that from sampling at different points in the distribution, a trend of higher scored posts relating to mental illness more often was observed. (These samples can be viewed in the appendices). After the application of these steps the data was tokenized and passed into the RoBERTa model to produce the feature vectors. A diagram illustating the pre-processing process can be seen below:

<img width="1300" alt="image" src="https://github.com/Adam-R26/ReddiKnowDense-Detecting-Signs-of-Mental-Illness-from-Reddit-Comments/assets/53123097/359f3ac9-29e0-48ca-9742-691cf8e71564">

#### Model Training and Evaluation:
- The models trained for this project involved traditional machine learning algorithms such as the Support Vector Classifer (SVC) and Extreme Gradient Boosting Models (XGBOOST) as well as modern the modern approaches fine tuning the RoBERTa pre-trainal neural architecture via adding Multi-Layer Perceptron (MLP) and Convolutional Neural Network (CNN) classification heads. Below is an in depth analysis of the results. 

<img width="1300" alt="image" src="https://github.com/Adam-R26/ReddiKnowDense-Detecting-Signs-of-Mental-Illness-from-Reddit-Comments/assets/53123097/6d193e5a-458b-4b1e-b1c6-23164dcd988d">

- Analyzing the out-of-bounds predictions further we note that despite setting the highest level of determinism (temperature=0) we still encounter them. Moreover, the number of out-of-bounds predictions generally increased as the number of examples provided via prompt engineering increased, with the number of out-of-bounds examples for zero shot, one shot and few shot learning being 10, 15 and 14. Upon analyzing the cause of this, we observe that when more training examples are provided within the prompt, the models predictions become more granular causing these out of bound predictions. For example, instead of simply predicting the class of self-harm as is stated in the initial prompt, some out-of-bounds predictions returned “suicide”. Some other common out-of-bounds predictions were for the schizophrenia and eating disorder classes, where labels of “paranoid”, “bodyimage” and “body” were returned instead of these classes. A further result to note is the performance of few-shot learning which appears to be anomalous. The cause of the decrease in performance between one-shot and few shot learning lies within the upper limit on prompts provided to GPT-3.5 (4096 tokens). This upper limit meant that as the size of the initial prompt grew (As more training examples were provided), the size of the remaining number of characters, that could be passed to the model shrunk. Since many of the user level submissions, contained more than the remaining number of characters they had to be trimmed, which was done via taking the first x number of characters from the submissions. Because of this, the chance of mental illness related linguistic signals lying within the shortened text decreased causing the drop in performance. A better approach would have been to fine-tune the GPT-3 Davinci variant, however this approach was infeasible due to insufficient funds as GPT-3 is currently a paid model. It is however hypothesized that the approach of few shot learning would outperform one-shot learning if this approach was employed.

- To conclude on this objective GPT-3.5 achieves state-of-the-art performance for the task of mental illness classification and would be considered the best method if it is not critical to avoid out of bounds predictions for the application scenario, and if one doesn’t have access to state-of-the-art hardware. This is because when comparing the performance of our models to a similar study in which they used an exponentially larger training set and unfreeze layers in the BERT model we see that higher results of 85.4% in F1 score were able to be achieved surpassing the results of GPT-3.5 (Jiang et al., 2020) . Another dimension to consider is the cost of model development. If the model will be used to make huge numbers of predictions on a regular basis for a long time period it would be better to invest in creating your own model using BERT based vectors due to the financial cost of using GPT-3.5 being tokens 
per request. However, if ones use case only requires the model to be used infrequently, using GPT-3.5 may be more economically viable, given that one doesn’t have to invest initial resources to create the model themselves and can consume it was ease, without having to worry about a commercial infrastructure set up.

#### Showcasing the Real World Application
- In order to showcase how the mental illness models could be leveraged to provide insights to medical professions in the real world, the following Power BI Dashboard was developed. It allows users to understand the different vocabulary used by people effected with different mental diseases, as well as the times of day that the most posts occur to help deduce if there is any relationships between time of day and the type of post user made.
- As well as this, it allows the most upvoted posts in each subreddit to be explored, and for the user to see how the frequency of mental illness related posts changes over time as a whole and for individual users, to try to understand any changes that may have happened that increased or decreased their posting of mental illness... And much more!

<img width="1300" alt="image" src="https://github.com/Adam-R26/ReddiKnowDense-Detecting-Signs-of-Mental-Illness-from-Reddit-Comments/assets/53123097/004ca0b6-da5a-41ae-a1ea-9d3409610358">

#### Software Design
- The design of the ML pipeline is below, there is also some discussion of the methodology used to create the pipeline:

![image](https://github.com/Adam-R26/ReddiKnowDense-Detecting-Signs-of-Mental-Illness-from-Reddit-Comments/assets/53123097/95d77d2a-7319-4955-8f7f-54481d0ab878)

- In industry the process of training machine learning models is done via machine learning pipelines. The idea behind these pipelines is to break the problem down into a number of smaller independent parts (high cohesion, low 
coupling classes) allowing different developers with different expertise to work on different parts of the pipeline in unison. For example, a larger company may have data engineers working on ensuring that the databases and other input streams involved within the pipeline are properly indexed, optimized, backed-up and secure. Data analysts working on writing SQL queries to extract and wrangle the data required for model training, and machine learning 
engineers working on taking the insights for modelling found by a data scientist, and productionalising them into code to train the classifiers, store them and allow predictions to be made from them via an API endpoint. For smaller 
companies who generally don’t have access to this amount of resource, the data scientist will often take on the role of machine learning engineer and the data analyst will take on the roles involved in data engineering.

- In motivating the purpose for these pipelines, each of the constituent parts involved within them can be discussed. The first step is for the required data to be extracted. This can come from a variety of sources for example SQL databases, web scraping, comma-separated value files etc. For this project our dataset is stored locally within a comma separated values file. This method was chosen due to the fact that a requirement on our data was that it 
needed to be easily disposed of after the project, hence there was no need for a database. The second step within apipeline is to perform data pre-processing to get the data into an appropriate format for modelling. The amount of 
pre- processing involved for a project depends heavily on the data acquisition method you have chosen. For example, if you scrape data from an inorganic source such as the web, then generally your data will include a lot of noise as it is
augmented for human consumption, and so lots of pre-processing would have to be done to remove symbols, split the input into different features etc. For this project our pre-processing steps are minimal due to the fact that as 
much context as possible needs to be preserved within the text, in order for high quality feature vectors to be obtained. The exact steps performed will be special character removal, normalizing domain-based noise, removing 
redundant whitespaces, removing deleted submissions and removing comments less than 50 characters in size. The third pipeline step is model building in which one initializes the models they wish to train and sets up any methods 
for optimization of model hyperparameters they would require such as k-folds cross validation. The final step is to evaluate the performance of your models, by benchmarking them on the test set and outputting these results. One may wish to do this through a series of tables and data visualizations comparing the accuracy and F1 score of your trained models. The pipeline itself would then return these trained models along with the performance metrics of 
each model. (Hapke & Nelson, 2020)

- From the above description of the data process four main steps can be identified namely, data acquisition, pre￾processing, model training and evaluation. Relating this to the unified modelling language (UML) design diagram in figure 1 it can be seen that a class has been assigned to each of these responsibilities. The Reddit data acquisition class has instance variables indicating the subreddits one wishes to attain data from, the time period for which they want the data, the amount of data to acquire and the path the data should be stored or read in from. It will be used to generate the raw dataset.

- The next class within the diagram is the ‘RedditDataPreprocessor’ class in which the aforementioned pre-processing steps will be applied to create high-quality training data. Note that this is where our method of self-reporting will be implemented via regular expression to extract self-diagnosed users. After this comes the ‘FeatureExtractorClass’, the purpose of which, is to use a pre-trained RoBERTa network to extract feature vectors for each post within the dataset, which will then be passed into the models used in this project.

- Next comes the ‘ModelTrainer’ class, which facilitates both model training and hyperparameter optimization for the traditional models. For the deep learning models, the same hyperparameters that have worked well in related literature will be used to ensure robustness in our results. As due to the computationally expensive training involved with deep learning solutions, it is infeasible for us to use cross validation to optimize their hyperparameters. After this comes the model evaluator class, where the performance metrics of each model (Accuracy, F1 Score, Precision, Recall) will be computed with respect to the test set. Finally, the ‘SelfTrainer’ class will be used to apply both the 
confidence only and heuristic based self-training techniques to the unlabeled data, as well as to re-train the models with the augmented training sets and benchmark them to understand the possible performance increases from self￾training. It therefore has composition relationships with both the ‘ModelTrainer’ and ‘ModelEvaluator’ classes in order to access these functionalities.

- The ‘Pipeline’ class is the control flow class that is responsible for passing the data to the relevant objects through the pipeline, and the ‘PipelineConfiguration’ and ‘HyperparamterConfigs’ classes are data classes used to encapsulate away the complexity of manually setting attributes throughout the pipeline and to ensure correct parameters are provided.

#### Bonus: Why are transformers so powerful?



#### Reference List
- Ansari, G., Garg, M. and Saxena, C. (2021) “Data Augmentation for Mental Health Classification on Social Media,” Proceedings of the 18th International Conference on Natural Language Processing (ICON), pp. 152–161. Available at: https://aclanthology.org/2021.icon-main.19 (Accessed: February 16, 2023).
- Banerjee, B. et al. (2015) “A new self-training-based unsupervised satellite image classification technique using Cluster Ensemble strategy,” IEEE Geoscience and Remote Sensing Letters, 12(4), pp. 741–745. Available at: https://doi.org/10.1109/lgrs.2014.2360833. 
- Browne, A. (2021) Getting the most out of GPT-3-based text classifiers: Part One, Medium. Edge Analytics. Available at: https://medium.com/edge-analytics/getting-the-most-out-of-gpt-3-based-text-classifiers-part-one-797460a5556e (Accessed: February 20, 2023). 
- Cavin, A. (2022) GPT-3 parameters and prompt design, Medium. Towards Data Science. Available at: https://towardsdatascience.com/gpt-3-parameters-and-prompt-design-1a595dc5b405 (Accessed: February 18, 2023). 
- Cai, T. et al. (2022) “Real-time classification of disaster images from social media with a self-supervised learning framework,” IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium [Preprint]. Available at: https://doi.org/10.1109/igarss46834.2022.9883129. 
- Chen, B. et al. (2022) “Debiased pseudo labeling in self-training.,” arXiv preprint arXiv:2202.07136. [Preprint]. Available at: https://doi.org/https://doi.org/10.48550/arXiv.2202.07136.
- Chen, Y. et al. (2021) “Self-training for domain adaptive scene text detection,” 2020 25th International Conference on Pattern Recognition (ICPR) [Preprint]. Available at: https://doi.org/10.1109/icpr48806.2021.9412558. 
- Chen, Z. et al. (2023) “Detecting Reddit Users with Depression Using a Hybrid Neural Network,” CoRR [Preprint]. Available at: https://doi.org/https://doi.org/10.48550/arXiv.2302.02759. 
- Chollet, F. et al. (2015). Keras. Available at: https://github.com/fchollet/keras.
- Cohan, A. et al. (2018) “SMHD : a large-scale resource for exploring online language usage for multiple mental health conditions,” Proceedings of the 27th International Conference on Computational Linguistics, pp. 1485–1497. Available at: https://aclanthology.org/C18-1126/ (Accessed: February 14, 2023).
- Companies using scikit-learn, market share, customers and competitors (2022) HG Insights. Available at: https://discovery.hgdata.com/product/scikit-learn (Accessed: October 25, 2022). 
- Coppersmith, G., Harman, C. and Dredze, M. (2014) “Measuring post-traumatic stress disorder in Twitter,” Proceedings of the International AAAI Conference on Web and Social Media, 8(1), pp. 579–582. Available at: https://doi.org/10.1609/icwsm.v8i1.14574. 
- Durgia, C. (2021) Exploring Bert variants (part 1): Albert, Roberta, electra, Medium. Towards Data Science. Available at: https://towardsdatascience.com/exploring-bert-variants-albert-roberta-electra-642dfe51bc23#:~:text=Dynamic%20Masking%3A%20BERT%20uses%20static,makes%20the%20model%20more%20robust. (Accessed: March 2, 2023). 
- Gkolfopoulos, G. and Varlamis, I. (2022) “Developing a news classifier for Greek using bert,” 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) [Preprint]. Available at: https://doi.org/10.1109/seeda-cecnsm57760.2022.9932996. 
- Gkotsis, G. et al. (2017) “Characterisation of mental health conditions in social media using informed Deep Learning,” Scientific Reports, 7(1). Available at: https://doi.org/10.1038/srep45141.
- HaCohen-Kerner, Y., Miller, D. and Yigal, Y. (2020) “The influence of preprocessing on text classification using a bag-of-words representation,” PLOS ONE, 15(5). Available at: https://doi.org/10.1371/journal.pone.0232525. 
- Hapke, H. and Nelson, C. (2020) Building Machine Learning Pipelines: Automating model life cycles with tensorflow. Beijing etc.: O'Reilly. 
- Huang, P. et al. (2021) “Text sentiment analysis based on Bert and Convolutional Neural Networks,” 2021 5th International Conference on Natural Language Processing and Information Retrieval (NLPIR) [Preprint]. Available at: https://doi.org/10.1145/3508230.3508231. 
- Hudgeon, D. and Nichol, R. (2020) Machine Learning for Business: Using amazon sagemaker and Jupyter, Amazon. Manning Publications Co. Available at: https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-HowItWorks.html (Accessed: March 1, 2023). 
- Jiang, Z. et al. (2020) “Detection of mental health from Reddit via deep contextualized representations,” Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis [Preprint]. Available at: https://doi.org/10.18653/v1/2020.louhi-1.16. 
- Kaliyar, R.K., Goswami, A. and Narang, P. (2021) “Fakebert: Fake news detection in social media with a Bert-based deep learning approach,” Multimedia Tools and Applications, 80(8), pp. 11765–11788. Available at: https://doi.org/10.1007/s11042-020-10183-2. 
- Kamal, M. et al. (2020) “Predicting mental illness using social media posts and comments,” International Journal of Advanced Computer Science and Applications, 11(12). Available at: https://doi.org/10.14569/ijacsa.2020.0111271. 
- Khyani, Divya & B S, Siddhartha. (2021). An Interpretation of Lemmatization and Stemming in Natural Language Processing. Shanghai Ligong Daxue Xuebao/Journal of University of Shanghai for Science and Technology. 22. 350-357.
- Kim, Y. (2014) “Convolutional Neural Networks for Sentence Classification”. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), [online] Available at: <https://arxiv.org/abs/1408.5882>. 
- Kirinde Gamaarachchige, P. and Inkpen, D. (2019) “Multi-task, multi-channel, multi-input learning for mental illness detection using social media text,” Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019) [Preprint]. Available at: https://doi.org/10.18653/v1/d19-6208. 
- Korngiebel, D.M. and Mooney, S.D. (2021) “Considering the possibilities and pitfalls of generative pre-trained transformer 3 (GPT-3) in healthcare delivery,” npj Digital Medicine, 4(1). Available at: https://doi.org/10.1038/s41746-021-00464-x. 
- Loveys, K. et al. (2017) “Small but mighty: Affective micropatterns for quantifying mental health from social media language,” Proceedings of the Fourth Workshop on Computational Linguistics and Clinical Psychology –- From Linguistic Signal to Clinical Reality [Preprint]. Available at: https://doi.org/10.18653/v1/w17-3110. 
- Ma, B. et al. (2020) “Semi-supervised sentence classification based on user polarity in the social scenarios,” ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) [Preprint]. Available at: https://doi.org/10.1109/icassp40776.2020.9053774. 
- Moe, M. and Oo, K. (2020). “Evaluation of Quality, Productivity, and Defect by applying Test-Driven Development to perform Unit Tests.” 2020 IEEE 9th Global Conference on Consumer Electronics (GCCE). Available at: https://ieeexplore.ieee.org/document/9291950.
- Murarka, A., Radhakrishnan, B. and Ravichandran, S. (2021) “Classification of Mental Illnesses on Social Media using RoBERTa,” Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis, pp. 59–68. Available at: https://aclanthology.org/2021.louhi-1.7 (Accessed: February 15, 2023). 
- Nasrullah, S. and Jalali, A. (2022) “Detection of types of mental illness through the social network using ensembled deep learning model,” Computational Intelligence and Neuroscience, 2022, pp. 1–6. Available at: https://doi.org/10.1155/2022/9404242. 
- Nayak, S. et al. (2022) “A machine learning approach to analyze mental health from reddit posts,” Biologically Inspired Techniques in Many Criteria Decision Making, pp. 357–366. Available at: https://doi.org/10.1007/978-981-16-8739-6_33. 
- NHS (2022) Mental Health of Children and Young People in England 2022 - Wave 3 Follow up to the 2017 survey, NHS choices. NHS. Available at: https://digital.nhs.uk/data-and-information/publications/statistical/mental-health-of-children-and-young-people-in-england/2022-follow-up-to-the-2017-survey (Accessed: February 24, 2023). 
- Park, S. and Kwak, N. (2017) “Analysis on the dropout effect in Convolutional Neural Networks,” Computer Vision – ACCV 2016, pp. 189–204. Available at: https://doi.org/10.1007/978-3-319-54184-6_12. 
- Potts, C. (2022) Roberta | Stanford CS224U Natural language understanding | spring 2021, YouTube. YouTube. Available at: https://www.youtube.com/watch?v=EZMOBbu_5b8&t=46s (Accessed: March 1, 2023). 
- Parsons, L. (2022) Poor Mental Health costs UK employers up to £56 billion a year, Deloitte United Kingdom. Available at: https://www2.deloitte.com/uk/en/pages/press-releases/articles/poor-mental-health-costs-uk-employers-up-to-pound-56-billion-a-year.html (Accessed: February 24, 2023). 
- Qasim, R. et al. (2022) “A fine-tuned BERT-based transfer learning approach for text classification,” Journal of Healthcare Engineering, 2022, pp. 1–17. Available at: https://doi.org/10.1155/2022/3498123. 
Soldevilla, I. and Flores, N. (2021) “Natural language processing through Bert for identifying gender-based violence messages on social media,” 2021 IEEE International Conference on Information Communication and Software Engineering (ICICSE) [Preprint]. Available at: https://doi.org/10.1109/icicse52190.2021.9404127. 
- Ventura, D. (2009). SVM Example, lecture notes. 
- Wang, J. et al. (2015) “Automatic Framework for semi-supervised Hyperspectral Image Classification Using Self-training with data editing,” 2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS) [Preprint]. Available at: https://doi.org/10.1109/whispers.2015.8075485. 
- World Health Organization (2022) 10 Facts on Mental Health, World Health Organization. World Health Organization. Available at: https://www.who.int/news-room/facts-in-pictures/detail/mental-health (Accessed: February 24, 2023). 
- Yan, D. and Balkus, S. (2022) “Improving Short Text Classification With Augmented Data Using GPT-3,” ArXiv, abs/2205.10981. Available at: https://doi.org/https://doi.org/10.48550/arXiv.2205.10981. 
- Yu, B., Deng, C. and Bu, L. (2022) “Policy text classification algorithm based on bert,” 2022 11th International Conference of Information and Communication Technology (ICTech)) [Preprint]. Available at: https://doi.org/10.1109/ictech55460.2022.00103. 
- Zhao, Z., Zhang, Z. and Hopfgartner, F. (2019) “Detecting toxic content online and the effect of training data on classification performance,” EasyChair Preprints [Preprint]. Available at: https://doi.org/10.29007/z5xk. 
- Zheng, Y. et al. (2008) “Text classification based on Transfer Learning and self-training,” 2008 Fourth International Conference on Natural Computation [Preprint]. Available at: https://doi.org/10.1109/icnc.2008.498. 


















